# -*- coding: utf-8 -*-
"""Jubaer_FloodPredictionAlgosipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lErz8yotSka2stNrdgQwOJGLVjZRDzoN
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import MinMaxScaler
from sklearn import metrics
from sklearn.metrics import confusion_matrix
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier

from google.colab import drive

drive.mount('/content/drive')

data = pd.read_csv("/content/drive/MyDrive/DATA/dataset_v3.csv")
data

data.drop(columns=["Unnamed: 0"], inplace = True)
data

data.drop(columns= ["Area_threshold"], inplace=True)
data

data.dropna(axis=0, inplace = True)

feature = data[["Rainfall", "Humidity", "Tmax", "Tmin", "Tavg", "Wind_spd", "Cloud_amt", "max_wl", "min_wl", 'avg_wl']]
label = data[["Flood"]]

stratY = pd.DataFrame(label)
XX=feature.values
YY=label.values
X_train, X_test, y_train, y_test = train_test_split(XX, YY, test_size = 0.25, random_state=0)

# stratY = pd.DataFrame(label)
# XX=data.loc[:, :'avg_wl'].values
# YY=data.loc[:, 'Flood'].values
# X_train, X_test, y_train, y_test = train_test_split(XX, YY, test_size = 0.25, random_state=0)

nan_values = label.isna().sum()
print(nan_values)

dtc = DecisionTreeClassifier(criterion='entropy',random_state=1)
dtc.fit(X_train,y_train)
y_pred_dtc = dtc.predict(X_test)

print("Accuracy for Decision Tree Classifier = {:.3f}".format(dtc.score(X_test, y_test)))
print("Error for Decision Tree Classifier = {:.3f}".format(1-dtc.score(X_test, y_test)))

y_pred_dtc = dtc.predict(X_train)
LABELS = ['Flood', 'No Flood']

conf_matrix = confusion_matrix(y_train, y_pred_dtc)

plt.figure(figsize =(12, 12))

sns.heatmap(conf_matrix, xticklabels = LABELS,

			yticklabels = LABELS, annot = True, fmt ="d");

#plt.title("Confusion matrix: [Decision Tree Classifier]")

plt.ylabel('True class')

plt.xlabel('Predicted class')
plt.rcParams.update({'font.size': 60})
plt.show()

#Dataset Biased towards no Flood
#Take Randomly Flood and no flood numbers for training dataset

# LABELS = ['Flood', 'No Flood']

# conf_matrix = confusion_matrix(y_test, y_pred_dtc)

# plt.figure(figsize =(12, 12))

# sns.heatmap(conf_matrix, xticklabels = LABELS,

# 			yticklabels = LABELS, annot = True, fmt ="d");

# #plt.title("Confusion matrix: [Decision Tree Classifier]")

# plt.ylabel('True class')

# plt.xlabel('Predicted class')
# plt.rcParams.update({'font.size': 60})
# plt.show()

# #Dataset Biased towards no Flood
# Lateef #Take Equal Flood and no flood numbers for training and test dataset but make sure all stations come in and different distrubuitions can come in
# Jubaer #Which stations has the most flood? \#EDA on parameters - which station how much % of the data? Spread on each parameter-remove outliers, ranges. How can we bring a fair distribution within the data to avoid overfitting
# Lateef - PCA - #Which factors were present but didn't result in flooding


# Toufique #Encode Data
# #Remove meaningless data from the data parameters that have low ranges

# ##-----Further Models to run---------
# #gradient boosting, xg boosting
# #SVM

# #CNN

# ##-----Finally------
# #Minimize overfitting (After EDA)
# #Bias Variance Tradeoff (Understanding False positives and False negatives on the data and how to minimize it)

knn = KNeighborsClassifier(n_neighbors=1)
knn.fit(X_train, y_train) #training
y_pred_knn = knn.predict(X_test)

print("Accuracy for KNeighbors Classifier = {:.3f}".format(knn.score(X_test, y_test)))
print("Error for KNeighbors Classifier = {:.3f}".format(1-knn.score(X_test, y_test)))

LABELS = ['Flood', 'No Flood']

conf_matrix = confusion_matrix(y_test, y_pred_knn)

plt.figure(figsize =(12, 12))

sns.heatmap(conf_matrix, xticklabels = LABELS,

			yticklabels = LABELS, annot = True, fmt ="d");

#plt.title("Confusion matrix: [K Nearest Neighbours Classifier]")

plt.ylabel('True class')

plt.xlabel('Predicted class')
plt.rcParams.update({'font.size': 60})
plt.show()

lrm = LogisticRegression(random_state=0)
lrm.fit(X_train, y_train)
y_pred_lrm = lrm.predict(X_test)

print("Accuracy for Logistic Regression = {0:.3f}".format(lrm.score(X_test, y_test)))
print("Error for Logistic Regression = {0:.3f}".format(1-lrm.score(X_test, y_test)))

LABELS = ['Flood', 'No Flood']

conf_matrix = confusion_matrix(y_test, y_pred_lrm)

plt.figure(figsize =(12, 12))

sns.heatmap(conf_matrix, xticklabels = LABELS,

			yticklabels = LABELS, annot = True, fmt ="d");

#plt.title("Confusion matrix: [Logistic Regression]")

plt.ylabel('True class')

plt.xlabel('Predicted class')
plt.rcParams.update({'font.size': 60})
plt.show()

# Assuming the dataframe 'data' contains information about stations and floods
# The following code will find the stations with the most floods

# Check if 'Station' and 'Flood' columns exist
if 'station' in data.columns and 'Flood' in data.columns:
    # Counting the number of floods per station
    flood_count_per_station = data.groupby('station')['Flood'].sum()

    # Finding the station(s) with the most floods
    most_floods_station = flood_count_per_station.idxmax()
    most_floods_count = flood_count_per_station.max()

    print("Station(s) with the most floods:", most_floods_station)
    print("Number of floods at this station:", most_floods_count)
else:
    print("The necessary columns 'station' and 'flood' are not found in the dataset.")

# Here is the number of flood occurrences for each station in your dataset:

# Bandarban: 11,836 floods
# Bogura: 12,276 floods
# Chattogram: 7,420 floods
# Chuadanga: 11,636 floods
# Coxs Bazar: 10,311 floods
# Dhaka: 0 floods
# Dinajpur: 12,276 floods
# Habiganj: 10,325 floods
# Jamalpur: 11,909 floods
# Mymensingh: 0 floods
# Netrokona: 1,860 floods
# Nilphamari: 0 floods
# Rajshahi: 12,183 floods
# Rangamati: 12,276 floods
# Rangpur: 12,276 floods
# Sirajganj: 11,144 floods
# Srimongal: 0 floods
# Sylhet: 11,595 floods
# Teknaf: 0 floods

import matplotlib.pyplot as plt
import seaborn as sns

# Plotting normal distribution graph for "Rainfall"
plt.figure(figsize=(10, 8))
sns.histplot(data['Rainfall'], kde=True, bins=30)
plt.title('Normal Distribution for Rainfall')
plt.xlabel('Rainfall')
plt.ylabel('Frequency')
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Plotting a histogram with a KDE for the "Rainfall" parameter
plt.figure(figsize=(10, 6))
sns.histplot(data['Rainfall'], kde=True, bins=30, color='blue')
plt.title('Normal Distribution for Rainfall')
plt.xlabel('Rainfall')
plt.ylabel('Frequency')
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Plotting a histogram with a KDE for the "Humidity" parameter
plt.figure(figsize=(10, 6))
sns.histplot(data['Humidity'], kde=True, bins=30, color='blue')
plt.title('Normal Distribution for Humidity')
plt.xlabel('Humidity')
plt.ylabel('Frequency')
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Plotting a histogram with a KDE for the "Tmax" parameter
plt.figure(figsize=(10, 6))
sns.histplot(data['Tmax'], kde=True, bins=30, color='blue')
plt.title('Normal Distribution for Tmax')
plt.xlabel('Tmax')
plt.ylabel('Frequency')
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Plotting a histogram with a KDE for the "Tmin" parameter
plt.figure(figsize=(10, 6))
sns.histplot(data['Tmin'], kde=True, bins=30, color='blue')
plt.title('Normal Distribution for Tmin')
plt.xlabel('Tmin')
plt.ylabel('Frequency')
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Plotting a histogram with a KDE for the "Tavg" parameter
plt.figure(figsize=(10, 6))
sns.histplot(data['Tavg'], kde=True, bins=30, color='blue')
plt.title('Normal Distribution for Tavg')
plt.xlabel('Tavg')
plt.ylabel('Frequency')
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Plotting a histogram with a KDE for the "Wind_spd" parameter
plt.figure(figsize=(10, 6))
sns.histplot(data['Wind_spd'], kde=True, bins=30, color='blue')
plt.title('Normal Distribution for Wind_spd')
plt.xlabel('Wind_spd')
plt.ylabel('Frequency')
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Plotting a histogram with a KDE for the "Cloud_amt" parameter
plt.figure(figsize=(10, 6))
sns.histplot(data['Cloud_amt'], kde=True, bins=30, color='blue')
plt.title('Normal Distribution for Cloud_amt')
plt.xlabel('Cloud_amt')
plt.ylabel('Frequency')
plt.show()

# import matplotlib.pyplot as plt
# import seaborn as sns

# # Plotting a histogram with a KDE for the "Area_threshold" parameter
# plt.figure(figsize=(10, 6))
# sns.histplot(data['Area_threshold'], kde=True, bins=30, color='blue')
# plt.title('Normal Distribution for Area_threshold')
# plt.xlabel('Area_threshold')
# plt.ylabel('Frequency')
# plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Plotting a histogram with a KDE for the "max_wl" parameter
plt.figure(figsize=(10, 6))
sns.histplot(data['max_wl'], kde=True, bins=30, color='blue')
plt.title('Normal Distribution for max_wl')
plt.xlabel('max_wl')
plt.ylabel('Frequency')
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Plotting a histogram with a KDE for the "min_wl" parameter
plt.figure(figsize=(10, 6))
sns.histplot(data['min_wl'], kde=True, bins=30, color='blue')
plt.title('Normal Distribution for min_wl')
plt.xlabel('min_wl')
plt.ylabel('Frequency')
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Plotting a histogram with a KDE for the "avg_wl" parameter
plt.figure(figsize=(10, 6))
sns.histplot(data['avg_wl'], kde=True, bins=30, color='blue')
plt.title('Normal Distribution for avg_wl')
plt.xlabel('avg_wl')
plt.ylabel('Frequency')
plt.show()

Q1 = data['Rainfall'].quantile(0.10)
Q3 = data['Rainfall'].quantile(0.90)
IQR = Q3 - Q1

lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

outliers = data[(data['Rainfall'] < lower_bound) | (data['Rainfall'] > upper_bound)]
outliers_count = outliers.shape[0]
print(outliers_count)
print(lower_bound)
print(upper_bound)
print(IQR)

Q1 = data['Humidity'].quantile(0.10)
Q3 = data['Humidity'].quantile(0.90)
IQR = Q3 - Q1

lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

outliers = data[(data['Humidity'] < lower_bound) | (data['Humidity'] > upper_bound)]
outliers_count = outliers.shape[0]
print(outliers_count)
print(lower_bound)
print(upper_bound)
print(IQR)

Q1 = data['Tmax'].quantile(0.10)
Q3 = data['Tmax'].quantile(0.90)
IQR = Q3 - Q1

lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

outliers = data[(data['Tmax'] < lower_bound) | (data['Tmax'] > upper_bound)]
outliers_count = outliers.shape[0]
print(outliers_count)
print(lower_bound)
print(upper_bound)
print(IQR)

Q1 = data['Tmin'].quantile(0.10)
Q3 = data['Tmin'].quantile(0.90)
IQR = Q3 - Q1

lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

outliers = data[(data['Tmin'] < lower_bound) | (data['Tmin'] > upper_bound)]
outliers_count = outliers.shape[0]
print(outliers_count)
print(lower_bound)
print(upper_bound)
print(IQR)

Q1 = data['Tavg'].quantile(0.10)
Q3 = data['Tavg'].quantile(0.90)
IQR = Q3 - Q1

lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

outliers = data[(data['Tavg'] < lower_bound) | (data['Tavg'] > upper_bound)]
outliers_count = outliers.shape[0]
print(outliers_count)
print(lower_bound)
print(upper_bound)
print(IQR)

Q1 = data['Wind_spd'].quantile(0.10)
Q3 = data['Wind_spd'].quantile(0.90)
IQR = Q3 - Q1

lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

outliers = data[(data['Wind_spd'] < lower_bound) | (data['Wind_spd'] > upper_bound)]
outliers_count = outliers.shape[0]
print(outliers_count)
print(lower_bound)
print(upper_bound)
print(IQR)

Q1 = data['Cloud_amt'].quantile(0.10)
Q3 = data['Cloud_amt'].quantile(0.90)
IQR = Q3 - Q1

lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

outliers = data[(data['Cloud_amt'] < lower_bound) | (data['Cloud_amt'] > upper_bound)]
outliers_count = outliers.shape[0]
print(outliers_count)
print(lower_bound)
print(upper_bound)
print(IQR)

# Q1 = data['Area_threshold'].quantile(0.10)
# Q3 = data['Area_threshold'].quantile(0.90)
# IQR = Q3 - Q1

# lower_bound = Q1 - 1.5 * IQR
# upper_bound = Q3 + 1.5 * IQR

# outliers = data[(data['Area_threshold'] < lower_bound) | (data['Area_threshold'] > upper_bound)]
# outliers_count = outliers.shape[0]
# print(outliers_count)
# print(lower_bound)
# print(upper_bound)
# print(IQR)

Q1 = data['max_wl'].quantile(0.10)
Q3 = data['max_wl'].quantile(0.90)
IQR = Q3 - Q1

lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

outliers = data[(data['max_wl'] < lower_bound) | (data['max_wl'] > upper_bound)]
outliers_count = outliers.shape[0]
print(outliers_count)
print(lower_bound)
print(upper_bound)
print(IQR)

Q1 = data['min_wl'].quantile(0.10)
Q3 = data['min_wl'].quantile(0.90)
IQR = Q3 - Q1

lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

outliers = data[(data['min_wl'] < lower_bound) | (data['min_wl'] > upper_bound)]
outliers_count = outliers.shape[0]
print(outliers_count)
print(lower_bound)
print(upper_bound)
print(IQR)

Q1 = data['avg_wl'].quantile(0.10)
Q3 = data['avg_wl'].quantile(0.90)
IQR = Q3 - Q1

lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

outliers = data[(data['avg_wl'] < lower_bound) | (data['avg_wl'] > upper_bound)]
outliers_count = outliers.shape[0]
print(outliers_count)
print(lower_bound)
print(upper_bound)
print(IQR)

from sklearn.preprocessing import LabelEncoder

# Creating an instance of label encoder
label_encoder = LabelEncoder()

# Encoding the 'Wind_dir' column
data['Wind_dir_encoded'] = label_encoder.fit_transform(data['Wind_dir'])

# Displaying the first few rows to see the encoded column
data[['Wind_dir', 'Wind_dir_encoded']].head()

print(data)

# from sklearn.preprocessing import OneHotEncoder
# import pandas as pd

# # Creating an instance of OneHotEncoder
# one_hot_encoder = OneHotEncoder(sparse=False)

# # Encoding the 'Wind_dir' column using One-Hot Encoding
# encoded_data = one_hot_encoder.fit_transform(data[['Wind_dir']])

# # Creating a DataFrame for the encoded data
# encoded_df = pd.DataFrame(encoded_data, columns=one_hot_encoder.get_feature_names(['Wind_dir']))

# # Concatenating the encoded DataFrame with the original DataFrame
# encoded_dataset = pd.concat([data, encoded_df], axis=1)

# # Displaying the first few rows of the new dataset
# encoded_dataset.head()

from sklearn.preprocessing import LabelEncoder

# Creating an instance of label encoder
label_encoder = LabelEncoder()

# Encoding the 'Wind_dir' column
data['Wind_dir_encoded'] = label_encoder.fit_transform(data['Wind_dir'])

# Displaying the first few rows to confirm the encoded column is part of the whole dataset
data.head()

# Displaying the first 20 rows of the dataset
data.head(20)