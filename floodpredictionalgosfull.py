# -*- coding: utf-8 -*-
"""FloodPredictionAlgosFull.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tD3ePYQwQhfmrZZ4F0YXCFIFoVSJ3l-K
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import MinMaxScaler
from sklearn import metrics
from sklearn.metrics import confusion_matrix
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier

data = pd.read_csv("dataset_v3.csv")
data

data.drop(columns=["Unnamed: 0"], inplace = True)
data

data.drop(columns= ["Area_threshold"], inplace=True)
data

data.dropna(axis=0, inplace = True)

feature = data[["Rainfall", "Humidity", "Tmax", "Tmin", "Tavg", "Wind_spd", "Cloud_amt", "max_wl", "min_wl", 'avg_wl']]
label = data[["Flood"]]

stratY = pd.DataFrame(label)
XX=feature.values
YY=label.values
X_train, X_test, y_train, y_test = train_test_split(XX, YY, test_size = 0.25, random_state=0)

# stratY = pd.DataFrame(label)
# XX=data.loc[:, :'avg_wl'].values
# YY=data.loc[:, 'Flood'].values
# X_train, X_test, y_train, y_test = train_test_split(XX, YY, test_size = 0.25, random_state=0)

nan_values = label.isna().sum()
print(nan_values)

dtc = DecisionTreeClassifier(criterion='entropy',random_state=1)
dtc.fit(X_train,y_train)
y_pred_dtc = dtc.predict(X_test)

print("Accuracy for Decision Tree Classifier = {:.3f}".format(dtc.score(X_test, y_test)))
print("Error for Decision Tree Classifier = {:.3f}".format(1-dtc.score(X_test, y_test)))

y_pred_dtc = dtc.predict(X_train)
LABELS = ['Flood', 'No Flood']

conf_matrix = confusion_matrix(y_train, y_pred_dtc)

plt.figure(figsize =(12, 12))

sns.heatmap(conf_matrix, xticklabels = LABELS,

			yticklabels = LABELS, annot = True, fmt ="d");

#plt.title("Confusion matrix: [Decision Tree Classifier]")

plt.ylabel('True class')

plt.xlabel('Predicted class')
plt.rcParams.update({'font.size': 60})
plt.show()

#Dataset Biased towards no Flood
#Take Randomly Flood and no flood numbers for training dataset

# LABELS = ['Flood', 'No Flood']

# conf_matrix = confusion_matrix(y_test, y_pred_dtc)

# plt.figure(figsize =(12, 12))

# sns.heatmap(conf_matrix, xticklabels = LABELS,

# 			yticklabels = LABELS, annot = True, fmt ="d");

# #plt.title("Confusion matrix: [Decision Tree Classifier]")

# plt.ylabel('True class')

# plt.xlabel('Predicted class')
# plt.rcParams.update({'font.size': 60})
# plt.show()

#Dataset Biased towards no Flood
Lateef #Take Equal Flood and no flood numbers for training and test dataset but make sure all stations come in and different distrubuitions can come in
Jubaer #Which stations has the most flood? \#EDA on parameters - which station how much % of the data? Spread on each parameter-remove outliers, ranges. How can we bring a fair distribution within the data to avoid overfitting
Lateef - PCA - #Which factors were present but didn't result in flooding


Toufique #Encode Data
#Remove meaningless data from the data parameters that have low ranges

##-----Further Models to run---------
#gradient boosting, xg boosting
#SVM

#CNN

##-----Finally------
#Minimize overfitting (After EDA)
#Bias Variance Tradeoff (Understanding False positives and False negatives on the data and how to minimize it)

knn = KNeighborsClassifier(n_neighbors=1)
knn.fit(X_train, y_train) #training
y_pred_knn = knn.predict(X_test)

print("Accuracy for KNeighbors Classifier = {:.3f}".format(knn.score(X_test, y_test)))
print("Error for KNeighbors Classifier = {:.3f}".format(1-knn.score(X_test, y_test)))

LABELS = ['Flood', 'No Flood']

conf_matrix = confusion_matrix(y_test, y_pred_knn)

plt.figure(figsize =(12, 12))

sns.heatmap(conf_matrix, xticklabels = LABELS,

			yticklabels = LABELS, annot = True, fmt ="d");

#plt.title("Confusion matrix: [K Nearest Neighbours Classifier]")

plt.ylabel('True class')

plt.xlabel('Predicted class')
plt.rcParams.update({'font.size': 60})
plt.show()

lrm = LogisticRegression(random_state=0)
lrm.fit(X_train, y_train)
y_pred_lrm = lrm.predict(X_test)

print("Accuracy for Logistic Regression = {0:.3f}".format(lrm.score(X_test, y_test)))
print("Error for Logistic Regression = {0:.3f}".format(1-lrm.score(X_test, y_test)))

LABELS = ['Flood', 'No Flood']

conf_matrix = confusion_matrix(y_test, y_pred_lrm)

plt.figure(figsize =(12, 12))

sns.heatmap(conf_matrix, xticklabels = LABELS,

			yticklabels = LABELS, annot = True, fmt ="d");

#plt.title("Confusion matrix: [Logistic Regression]")

plt.ylabel('True class')

plt.xlabel('Predicted class')
plt.rcParams.update({'font.size': 60})
plt.show()

# Logistic regression
# Naive Bayes
# Support vector machine
# ada boosting
# X ai
# pca